{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIN9BXVknNqD+ImVfHMZgy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/premtodev08710/Sport_Management_System/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlfV097jAgKR",
        "outputId": "4471de54-8cb2-460d-d1c5-9ffe1930e8bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world welcome to nlp 101\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# ข้อมูลตัวอย่าง\n",
        "text = \"Hello, World! Welcome to NLP 101.\"\n",
        "\n",
        "# แปลงเป็นตัวพิมพ์เล็ก\n",
        "text = text.lower()\n",
        "\n",
        "# ลบเครื่องหมายพิเศษ\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "print(text)  # Output: hello world welcome to nlp 101\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# ตัวอย่าง DataFrame\n",
        "data = {'text': ['Hello world', None, 'Natural Language Processing']}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# แทนที่ค่าที่หายไปด้วย 'Unknown'\n",
        "df['text'] = df['text'].fillna('Unknown')\n",
        "\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SzeNyfNaArFX",
        "outputId": "b6c7a983-10d7-4418-c667-a65ab2d75088"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                          text\n",
            "0                  Hello world\n",
            "1                      Unknown\n",
            "2  Natural Language Processing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ติดตั้งและ Import NLTK\n",
        "ติดตั้ง NLTK:"
      ],
      "metadata": {
        "id": "xNgGesfRP-_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "# Import NLTK และดาวน์โหลดข้อมูลที่จำเป็น:\n",
        "import nltk\n",
        "nltk.download(\"all\")"
      ],
      "metadata": {
        "id": "qO82RfzjPUOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**\n",
        "1. Word Tokenization:\n",
        "การตัดข้อความออกเป็นคำ (Word)"
      ],
      "metadata": {
        "id": "Ht5iZb3GQIKW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text = \"Hello Mr. Smith, how are you doing today? The weather is great and Python is awesome. The sky is pinkish-blue. You should not eat cardboard.\"\n",
        "words = word_tokenize(text)\n",
        "print(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NBueQgrzPoIF",
        "outputId": "f8902085-6fb1-4fdc-c5ad-5bfb294faf16"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', 'not', 'eat', 'cardboard', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sentence Tokenization:\n",
        "การตัดข้อความออกเป็นประโยค (Sentence)"
      ],
      "metadata": {
        "id": "geBQ11tcQPuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization\n",
        "การแปลงคำให้อยู่ในรูปพื้นฐาน (Lemma)\n",
        "\n",
        "1. สร้างรายการคำ:"
      ],
      "metadata": {
        "id": "Cp2H1mrcQZd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "list_words = [\"playing\", \"plays\", \"played\", \"play\", \"is\", \"am\", \"are\", \"be\", \"goose\", \"geese\", \"mouse\", \"mice\"]\n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "Ez8x8v6BPgDc"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. แปลงคำประเภท Verb:"
      ],
      "metadata": {
        "id": "e75RVVzzQ0de"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "verbs = [lemmatizer.lemmatize(word, pos=wordnet.VERB) for word in list_words]\n",
        "print(verbs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B-GtK_eWPzgs",
        "outputId": "84a4654a-1201-4633-c10c-fe5a834e5b40"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play', 'play', 'be', 'be', 'be', 'be', 'goose', 'geese', 'mouse', 'mice']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. แปลงคำประเภท Noun:"
      ],
      "metadata": {
        "id": "SqmqYZEYQ5Bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nouns = [lemmatizer.lemmatize(word, pos=wordnet.NOUN) for word in list_words]\n",
        "print(nouns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "86cZNLK5PrvI",
        "outputId": "a8624b0d-ce25-4ca5-c184-acafc80fb43c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['playing', 'play', 'played', 'play', 'is', 'am', 'are', 'be', 'goose', 'goose', 'mouse', 'mouse']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**THAI**"
      ],
      "metadata": {
        "id": "wLXZWpcnRg3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q python-crfsuite\n",
        "!pip install -q torch\n",
        "!pip install -q pythainlp\n",
        "!pip install -q epitran"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4jeRu9fkRgXi",
        "outputId": "d13fe3fc-663c-45aa-8367-1c69e80c553e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/1.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m1.0/1.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.1/184.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m577.1/577.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.8/493.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for unicodecsv (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yV3qwOO-ReBz"
      },
      "source": [
        "## Import PyThaiNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "kjTj_2S6ReB0",
        "outputId": "b087d23f-0d4b-456c-b4d3-ab9cc31a99c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5.0.5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "import pythainlp\n",
        "\n",
        "pythainlp.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GAvoeZg3Igp-",
        "outputId": "1c5ee3c5-216c-4b8f-bb7d-b6ac035a704e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮฤฦะัาำิีึืุูเแโใไๅํ็่้๊๋ฯฺๆ์ํ๎๏๚๛๐๑๒๓๔๕๖๗๘๙฿'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ],
      "source": [
        "pythainlp.thai_characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "4OFEl2dtReB2",
        "outputId": "17d74afe-ea52-40c1-c860-0d43c20e3318",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "len(pythainlp.thai_characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uPwx53A6IgqF",
        "outputId": "b30fcf72-6849-4f40-91df-93dc6399bc73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'กขฃคฅฆงจฉชซฌญฎฏฐฑฒณดตถทธนบปผฝพฟภมยรลวศษสหฬอฮ'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "pythainlp.thai_consonants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "QOT01Uy1ReB4",
        "outputId": "e0d84ee3-82cd-4a21-9a0f-0c6e2ddf57db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "len(pythainlp.thai_consonants)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5UA7Hwy_IgqI",
        "outputId": "f6c15b6f-a27c-4ee2-a040-aebe838600cd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "\"๔\" in pythainlp.thai_digits  # check if Thai digit \"4\" is in the character set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StgsPgGRReB5"
      },
      "source": [
        "## Checking if a string contains Thai character or not, or how many"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "t3NvXqYFIgqK",
        "outputId": "df7e7d9c-2a58-4215-f73c-b8c84052c3ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "import pythainlp.util\n",
        "\n",
        "pythainlp.util.isthai(\"ก\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sRzSQjugIgqM",
        "outputId": "b2aafef6-6d8e-4559-8969-bc916266ccc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DP5yfJebIgqP",
        "outputId": "ebb36a0f-646b-4cf0-84d4-3ccf86aa20ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "pythainlp.util.isthai(\"(ก.พ.)\", ignore_chars=\".()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r83nl943ReB6"
      },
      "source": [
        "`counthai()` returns proportion of Thai characters in the text. It will ignore non-alphabets by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "87Z8P9WPIgqS",
        "outputId": "805fb400-c84a-4b61-8a05-8a39fe90bdd3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100.0"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "pythainlp.util.countthai(\"วันอาทิตย์ที่ 24 มีนาคม 2562\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaMSytheReB6"
      },
      "source": [
        "You can specify characters to be ignored, using `ignore_chars=` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ukSQP8ZTIgqV",
        "outputId": "c08e52c0-9f75-4dfc-eb8a-bead89ee05a8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "67.85714285714286"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "pythainlp.util.countthai(\"วันอาทิตย์ที่ 24 มีนาคม 2562\", ignore_chars=\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VFPOHyZIgqh"
      },
      "source": [
        "## Tokenization and Segmentation\n",
        "\n",
        "At sentence, word, and sub-word levels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tq_LX6cReB9"
      },
      "source": [
        "### Sentence\n",
        "\n",
        "Default sentence tokenizer is \"crfcut\". Tokenization engine can be chosen ussing `engine=` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "dFJ56eUuReB9",
        "outputId": "90d23675-0d64-44d1-ecea-2499656abb44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default (crfcut):\n",
            "['พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว พุทธศักราช ๒๔๗๕ เป็นรัฐธรรมนูญฉบับชั่วคราว ', 'ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม ', 'ประกาศใช้เมื่อวันที่ 27 มิถุนายน พ.ศ.', ' 2475 โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่ 24 มิถุนายน พ.ศ.', ' 2475 โดยคณะราษฎร']\n",
            "\n",
            "whitespace+newline:\n",
            "['พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว', 'พุทธศักราช', '๒๔๗๕', 'เป็นรัฐธรรมนูญฉบับชั่วคราว', 'ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม', 'ประกาศใช้เมื่อวันที่', '27', 'มิถุนายน', 'พ.ศ.', '2475', 'โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่', '24', 'มิถุนายน', 'พ.ศ.', '2475', 'โดยคณะราษฎร']\n"
          ]
        }
      ],
      "source": [
        "from pythainlp import sent_tokenize\n",
        "\n",
        "text = (\"พระราชบัญญัติธรรมนูญการปกครองแผ่นดินสยามชั่วคราว พุทธศักราช ๒๔๗๕ \"\n",
        "        \"เป็นรัฐธรรมนูญฉบับชั่วคราว ซึ่งถือว่าเป็นรัฐธรรมนูญฉบับแรกแห่งราชอาณาจักรสยาม \"\n",
        "        \"ประกาศใช้เมื่อวันที่ 27 มิถุนายน พ.ศ. 2475 \"\n",
        "        \"โดยเป็นผลพวงหลังการปฏิวัติเมื่อวันที่ 24 มิถุนายน พ.ศ. 2475 โดยคณะราษฎร\")\n",
        "\n",
        "print(\"default (crfcut):\")\n",
        "print(sent_tokenize(text))\n",
        "print(\"\\nwhitespace+newline:\")\n",
        "print(sent_tokenize(text, engine=\"whitespace+newline\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SklPJ-DbIgqi"
      },
      "source": [
        "### Word\n",
        "Default word tokenizer (\"newmm\") use maximum matching algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JEbY-MGCIgqi",
        "outputId": "8a9bd63f-4260-43c0-ef76-c261b73be6c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default (newmm):\n",
            "['ก็', 'จะ', 'รู้ความ', 'ชั่วร้าย', 'ที่', 'ทำ', 'ไว้', '     ', 'และ', 'คงจะ', 'ไม่', 'ยอมให้', 'ทำนาบนหลังคน', ' ']\n",
            "\n",
            "newmm and keep_whitespace=False:\n",
            "['ก็', 'จะ', 'รู้ความ', 'ชั่วร้าย', 'ที่', 'ทำ', 'ไว้', 'และ', 'คงจะ', 'ไม่', 'ยอมให้', 'ทำนาบนหลังคน']\n"
          ]
        }
      ],
      "source": [
        "from pythainlp import word_tokenize\n",
        "\n",
        "text = \"ก็จะรู้ความชั่วร้ายที่ทำไว้     และคงจะไม่ยอมให้ทำนาบนหลังคน \"\n",
        "\n",
        "print(\"default (newmm):\")\n",
        "print(word_tokenize(text))\n",
        "print(\"\\nnewmm and keep_whitespace=False:\")\n",
        "print(word_tokenize(text, keep_whitespace=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5P_YygrIgqm"
      },
      "source": [
        "Other algorithm can be chosen. We can also create a tokenizer with a custom dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mI_Qz3k3Igqm",
        "outputId": "b0677ce2-01b7-45f7-9445-4b10147e19ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "newmm  : ['กฎหมายแรงงาน', 'ฉบับ', 'ปรับปรุง', 'ใหม่', 'ประกาศ', 'ใช้แล้ว']\n",
            "longest: ['กฎหมายแรงงาน', 'ฉบับ', 'ปรับปรุง', 'ใหม่', 'ประกาศใช้', 'แล้ว']\n",
            "newmm (custom dictionary): ['กฎหมาย', 'แรงงาน', 'ฉบับปรับปรุงใหม่ประกาศใช้แล้ว']\n"
          ]
        }
      ],
      "source": [
        "from pythainlp import word_tokenize, Tokenizer\n",
        "\n",
        "text = \"กฎหมายแรงงานฉบับปรับปรุงใหม่ประกาศใช้แล้ว\"\n",
        "\n",
        "print(\"newmm  :\", word_tokenize(text))  # default engine is \"newmm\"\n",
        "print(\"longest:\", word_tokenize(text, engine=\"longest\"))\n",
        "\n",
        "words = [\"แรงงาน\"]\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"newmm (custom dictionary):\", custom_tokenizer.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIXUxXlTIgqo"
      },
      "source": [
        "Default word tokenizer use a word list from `pythainlp.corpus.common.thai_words()`.\n",
        "We can get that list, add/remove words, and create new tokenizer from the modified list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RblqNckGIgqp",
        "outputId": "929d4795-49b9-4440-a423-0c6c30d6f345"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['นิยาย', 'วิทยาศาสตร์', 'ของ', 'ไอแซค', ' ', 'อสิ', 'มอ', 'ฟ']\n",
            "custom dictionary : ['นิยาย', 'วิทยาศาสตร์', 'ของ', 'ไอแซค', ' ', 'อสิมอฟ']\n"
          ]
        }
      ],
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp import Tokenizer\n",
        "\n",
        "text = \"นิยายวิทยาศาสตร์ของไอแซค อสิมอฟ\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "words = set(thai_words())  # thai_words() returns frozenset\n",
        "words.add(\"ไอแซค\")  # Isaac\n",
        "words.add(\"อสิมอฟ\")  # Asimov\n",
        "custom_tokenizer = Tokenizer(words)\n",
        "print(\"custom dictionary :\", custom_tokenizer.word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmfUK2m2ReCD"
      },
      "source": [
        "We can also, alternatively, create a dictionary trie, using `pythainlp.util.Trie()` function, and pass it to a default tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "MJ4ycwWyReCD",
        "outputId": "294d221a-32f4-4ac3-a75e-e2d4876a8691",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "default dictionary: ['ILO', '87', ' ', 'ว่าด้วย', 'เสรีภาพ', 'ใน', 'การสมาคม', 'และ', 'การ', 'คุ้มครอง', 'สิทธิ', 'ใน', 'การ', 'รวมตัว', ' ', 'ILO', '98', ' ', 'ว่าด้วย', 'สิทธิ', 'ใน', 'การ', 'รวมตัว', 'และ', 'การ', 'ร่วม', 'เจรจา', 'ต่อรอง']\n",
            "custom dictionary : ['ILO87', ' ', 'ว่าด้วย', 'เสรีภาพในการสมาคม', 'และ', 'การ', 'คุ้มครอง', 'สิทธิในการรวมตัว', ' ', 'ILO98', ' ', 'ว่าด้วย', 'สิทธิในการรวมตัว', 'และ', 'การร่วมเจรจาต่อรอง']\n"
          ]
        }
      ],
      "source": [
        "from pythainlp.corpus.common import thai_words\n",
        "from pythainlp.util import Trie\n",
        "\n",
        "text = \"ILO87 ว่าด้วยเสรีภาพในการสมาคมและการคุ้มครองสิทธิในการรวมตัว ILO98 ว่าด้วยสิทธิในการรวมตัวและการร่วมเจรจาต่อรอง\"\n",
        "\n",
        "print(\"default dictionary:\", word_tokenize(text))\n",
        "\n",
        "new_words = {\"ILO87\", \"ILO98\", \"การร่วมเจรจาต่อรอง\", \"สิทธิในการรวมตัว\", \"เสรีภาพในการสมาคม\", \"แรงงานสัมพันธ์\"}\n",
        "words = new_words.union(thai_words())\n",
        "\n",
        "custom_dictionary_trie = Trie(words)\n",
        "print(\"custom dictionary :\", word_tokenize(text, custom_dict=custom_dictionary_trie))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avPSM9JQReCD"
      },
      "source": [
        "Testing different tokenization engines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "4L2kRMY5Igqr"
      },
      "outputs": [],
      "source": [
        "speedtest_text = \"\"\"\n",
        "ครบรอบ 14 ปี ตากใบ เช้าวันนั้น 25 ต.ค. 2547 ผู้ชุมนุมชายกว่า 1,370 คน\n",
        "ถูกโยนขึ้นรถยีเอ็มซี 22 หรือ 24 คัน นอนซ้อนกันคันละ 4-5 ชั้น เดินทางจากสถานีตำรวจตากใบ ไปไกล 150 กิโลเมตร\n",
        "ไปถึงค่ายอิงคยุทธบริหาร ใช้เวลากว่า 6 ชั่วโมง / ในอีกคดีที่ญาติฟ้องร้องรัฐ คดีจบลงที่การประนีประนอมยอมความ\n",
        "กระทรวงกลาโหมจ่ายค่าสินไหมทดแทนรวม 42 ล้านบาทให้กับญาติผู้เสียหาย 79 ราย\n",
        "ปิดหีบและนับคะแนนเสร็จแล้ว ที่หน่วยเลือกตั้งที่ 32 เขต 13 แขวงหัวหมาก เขตบางกะปิ กรุงเทพมหานคร\n",
        "ผู้สมัคร ส.ส. และตัวแทนพรรคการเมืองจากหลายพรรคต่างมาเฝ้าสังเกตการนับคะแนนอย่างใกล้ชิด โดย\n",
        "ฐิติภัสร์ โชติเดชาชัยนันต์ จากพรรคพลังประชารัฐ และพริษฐ์ วัชรสินธุ จากพรรคประชาธิปัตย์ได้คะแนน\n",
        "96 คะแนนเท่ากัน\n",
        "เช้าวันอาทิตย์ที่ 21 เมษายน 2019 ซึ่งเป็นวันอีสเตอร์ วันสำคัญของชาวคริสต์\n",
        "เกิดเหตุระเบิดต่อเนื่องในโบสถ์คริสต์และโรงแรมอย่างน้อย 7 แห่งในประเทศศรีลังกา\n",
        "มีผู้เสียชีวิตแล้วอย่างน้อย 156 คน และบาดเจ็บหลายร้อยคน ยังไม่มีข้อมูลว่าผู้ก่อเหตุมาจากฝ่ายใด\n",
        "จีนกำหนดจัดการประชุมข้อริเริ่มสายแถบและเส้นทางในช่วงปลายสัปดาห์นี้ ปักกิ่งยืนยันว่า\n",
        "อภิมหาโครงการเชื่อมโลกของจีนไม่ใช่เครื่องมือแผ่อิทธิพล แต่ยินดีรับฟังข้อวิจารณ์ เช่น ประเด็นกับดักหนี้สิน\n",
        "และความไม่โปร่งใส รัฐบาลปักกิ่งบอกว่า เวทีประชุม Belt and Road Forum ในช่วงวันที่ 25-27 เมษายน\n",
        "ถือเป็นงานการทูตที่สำคัญที่สุดของจีนในปี 2019\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiE0w2gJReCE"
      },
      "source": [
        "Get all possible segmentations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qFTYqAB1Igq1",
        "outputId": "bf0589ee-4843-4d9e-dd98-ac225b389a8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['มี|ความ|เป็น|ไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่าง|ไร|บ้าง|',\n",
              " 'มี|ความ|เป็น|ไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่างไร|บ้าง|',\n",
              " 'มี|ความ|เป็น|ไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความ|เป็นไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความ|เป็นไปได้|อย่างไรบ้าง|',\n",
              " 'มี|ความเป็นไป|ได้|อย่างไรบ้าง|',\n",
              " 'มี|ความเป็นไปได้|อย่างไรบ้าง|']"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ],
      "source": [
        "from pythainlp.tokenize.multi_cut import find_all_segment, mmcut, segment\n",
        "\n",
        "find_all_segment(\"มีความเป็นไปได้อย่างไรบ้าง\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWXiNUoBIgq4"
      },
      "source": [
        "### Subword, syllable, and Thai Character Cluster (TCC)\n",
        "\n",
        "Tokenization can also be done at subword level, either syllable or Thai Character Cluster (TCC).\n",
        "\n",
        "- Syllable segmentation is using [`ssg`](https://github.com/ponrawee/ssg), a CRF syllable segmenter for Thai by Ponrawee Prasertsom.\n",
        "- TCC is smaller than syllable. For information about TCC, see [Character Cluster Based Thai Information Retrieval](https://www.researchgate.net/publication/2853284_Character_Cluster_Based_Thai_Information_Retrieval) (Theeramunkong et al. 2004)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkqL6FAyReCF"
      },
      "source": [
        "#### Subword tokenization\n",
        "Default subword tokenization engine is `tcc`, which will use Thai Character Cluster (TCC) as a subword unit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z1wdMmdmIgq6",
        "outputId": "a61d8f49-2b53-4ccd-fdf8-f6395e9ffb03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ป', 'ระ', 'เท', 'ศ', 'ไท', 'ย']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "from pythainlp import subword_tokenize\n",
        "\n",
        "subword_tokenize(\"ประเทศไทย\")  # default subword unit is TCC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAASpNTLReCF"
      },
      "source": [
        "#### Syllable tokenization\n",
        "Default syllable tokenization engine is `dict`, which will use `newmm` word tokenization engine with a custom dictionary contains known syllables in Thai language."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "bM7RfZIqReCF",
        "outputId": "45ff5f12-7956-4895-9d74-22c3ee6c1b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['อับ',\n",
              " 'ดุล',\n",
              " 'เลาะ',\n",
              " ' ',\n",
              " 'อี',\n",
              " 'ซอ',\n",
              " 'มู',\n",
              " 'ซอ',\n",
              " ' ',\n",
              " 'สมอง',\n",
              " 'บวม',\n",
              " 'รุน',\n",
              " 'แรง']"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "from pythainlp.tokenize import syllable_tokenize\n",
        "\n",
        "text = \"อับดุลเลาะ อีซอมูซอ สมองบวมรุนแรง\"\n",
        "\n",
        "syllable_tokenize(text)  # default engine is \"dict\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iS7iwPoiIgr3"
      },
      "source": [
        "## Number Spell Out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "F9PEEvWLIgr4",
        "outputId": "5b410e15-75f2-44d5-96af-beca16583a44"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'หนึ่งล้านสองแสนสามหมื่นสี่พันห้าร้อยหกสิบเจ็ดล้านแปดแสนเก้าหมื่นหนึ่งร้อยยี่สิบสามบาทสี่สิบห้าสตางค์'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ],
      "source": [
        "from pythainlp.util import bahttext\n",
        "\n",
        "bahttext(1234567890123.45)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKqMoQ0qReCN"
      },
      "source": [
        "`bahttext()` will round the satang part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y6DLJYOEIgr7",
        "outputId": "2fd2a6b5-96b6-4b88-a632-e59031e278c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'หนึ่งบาทเก้าสิบเอ็ดสตางค์'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 106
        }
      ],
      "source": [
        "bahttext(1.909)"
      ]
    }
  ]
}